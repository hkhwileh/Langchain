{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U langchain langchain-huggingface langchain_community chromadb faiss-cpu transformers accelerate bitsandbytes langchain_core bs4 pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:02:56.233616Z",
     "iopub.status.busy": "2024-06-15T11:02:56.232607Z",
     "iopub.status.idle": "2024-06-15T11:02:56.240529Z",
     "shell.execute_reply": "2024-06-15T11:02:56.239639Z",
     "shell.execute_reply.started": "2024-06-15T11:02:56.233583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pypdf\n",
    "from langchain_community.document_loaders import WebBaseLoader,PyMuPDFLoader # Data Ingestion\n",
    "import bs4 # Beautiful Soup for webscraping\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #Document split and create chunks\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # Convert Doc into Vectors\n",
    "from langchain.vectorstores import Chroma # Vector Database to store vectors / docs\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig,pipeline # To load model and tokenizer\n",
    "import torch\n",
    "from langchain_huggingface import HuggingFacePipeline # To Create Huggingface pipeline with langchain to create LLM Model\n",
    "from langchain.chains import RetrievalQA # To make Vector DB as Retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate,PromptTemplate # To write prompt and template\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain # To combine LLM and Prompt and create chain\n",
    "from langchain.chains import create_retrieval_chain #To combine retriever and document chain for inferencing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:02:58.106873Z",
     "iopub.status.busy": "2024-06-15T11:02:58.106085Z",
     "iopub.status.idle": "2024-06-15T11:02:58.132779Z",
     "shell.execute_reply": "2024-06-15T11:02:58.131915Z",
     "shell.execute_reply.started": "2024-06-15T11:02:58.10684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:03:09.068067Z",
     "iopub.status.busy": "2024-06-15T11:03:09.067674Z",
     "iopub.status.idle": "2024-06-15T11:03:09.303725Z",
     "shell.execute_reply": "2024-06-15T11:03:09.302909Z",
     "shell.execute_reply.started": "2024-06-15T11:03:09.068039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_document= PyMuPDFLoader('/kaggle/input/attention-research-paper/NIPS-2017-attention-is-all-you-need-Paper.pdf').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:03:09.49999Z",
     "iopub.status.busy": "2024-06-15T11:03:09.499429Z",
     "iopub.status.idle": "2024-06-15T11:03:09.507036Z",
     "shell.execute_reply": "2024-06-15T11:03:09.506079Z",
     "shell.execute_reply.started": "2024-06-15T11:03:09.499948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split Documents in chunks\n",
    "text_splitter= RecursiveCharacterTextSplitter(chunk_size=3000,chunk_overlap=100)\n",
    "documents=text_splitter.split_documents(text_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:03:09.842938Z",
     "iopub.status.busy": "2024-06-15T11:03:09.842313Z",
     "iopub.status.idle": "2024-06-15T11:03:16.451177Z",
     "shell.execute_reply": "2024-06-15T11:03:16.450328Z",
     "shell.execute_reply.started": "2024-06-15T11:03:09.842909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create Embeddings\n",
    "embedding_model_name= \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings= HuggingFaceEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:03:16.453149Z",
     "iopub.status.busy": "2024-06-15T11:03:16.452841Z",
     "iopub.status.idle": "2024-06-15T11:03:19.040469Z",
     "shell.execute_reply": "2024-06-15T11:03:19.039595Z",
     "shell.execute_reply.started": "2024-06-15T11:03:16.453125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create Vector DB, Store document and Embeddings in DB\n",
    "db= Chroma.from_documents(documents=documents,embedding=embeddings,persist_directory='chroma_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:08:33.250669Z",
     "iopub.status.busy": "2024-06-15T11:08:33.250053Z",
     "iopub.status.idle": "2024-06-15T11:12:19.620449Z",
     "shell.execute_reply": "2024-06-15T11:12:19.619455Z",
     "shell.execute_reply.started": "2024-06-15T11:08:33.250639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Quantization and Load Model & Tokenizer\n",
    "bnb_config =BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Shorya22/LLaMA-2-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Shorya22/LLaMA-2-7B\",quantization_config=bnb_config,device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:13:43.36092Z",
     "iopub.status.busy": "2024-06-15T11:13:43.360081Z",
     "iopub.status.idle": "2024-06-15T11:13:43.366079Z",
     "shell.execute_reply": "2024-06-15T11:13:43.365194Z",
     "shell.execute_reply.started": "2024-06-15T11:13:43.360891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline using transformers\n",
    "pipe= pipeline(task='text-generation',model=model,tokenizer=tokenizer,max_new_tokens=512,temperature=0.3,do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:13:43.685062Z",
     "iopub.status.busy": "2024-06-15T11:13:43.684374Z",
     "iopub.status.idle": "2024-06-15T11:13:43.690479Z",
     "shell.execute_reply": "2024-06-15T11:13:43.689537Z",
     "shell.execute_reply.started": "2024-06-15T11:13:43.685032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# craete llm using Huggingface Pipeline\n",
    "llm= HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Without Prompt Template and Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:13:44.21282Z",
     "iopub.status.busy": "2024-06-15T11:13:44.212464Z",
     "iopub.status.idle": "2024-06-15T11:13:44.490425Z",
     "shell.execute_reply": "2024-06-15T11:13:44.489636Z",
     "shell.execute_reply.started": "2024-06-15T11:13:44.212791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create retriever for query to model/llm\n",
    "retriever = db.as_retriever()\n",
    "qa= RetrievalQA.from_chain_type(llm=llm,retriever=retriever,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:13:44.579601Z",
     "iopub.status.busy": "2024-06-15T11:13:44.579257Z",
     "iopub.status.idle": "2024-06-15T11:14:21.075233Z",
     "shell.execute_reply": "2024-06-15T11:14:21.074235Z",
     "shell.execute_reply.started": "2024-06-15T11:13:44.579572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inferencing\n",
    "result=qa.run('What is attention?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:14:21.077901Z",
     "iopub.status.busy": "2024-06-15T11:14:21.077296Z",
     "iopub.status.idle": "2024-06-15T11:14:21.082864Z",
     "shell.execute_reply": "2024-06-15T11:14:21.082016Z",
     "shell.execute_reply.started": "2024-06-15T11:14:21.077866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Answer:',result.split('Helpful Answer:')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline + Prompt Template + LLM Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:14:21.084527Z",
     "iopub.status.busy": "2024-06-15T11:14:21.084113Z",
     "iopub.status.idle": "2024-06-15T11:14:21.095903Z",
     "shell.execute_reply": "2024-06-15T11:14:21.095122Z",
     "shell.execute_reply.started": "2024-06-15T11:14:21.084497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Provide answer in bullet Points.\n",
    "Always end the answer with \"Thanks for asking!\".\n",
    "\n",
    "Context: {context}\\n\\n\\n\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['context', 'input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:14:21.098174Z",
     "iopub.status.busy": "2024-06-15T11:14:21.097882Z",
     "iopub.status.idle": "2024-06-15T11:14:21.107023Z",
     "shell.execute_reply": "2024-06-15T11:14:21.106176Z",
     "shell.execute_reply.started": "2024-06-15T11:14:21.098152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create LLM Document Chain and merge llm and prompt\n",
    "document_chain= create_stuff_documents_chain(llm=llm,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:14:21.108255Z",
     "iopub.status.busy": "2024-06-15T11:14:21.10799Z",
     "iopub.status.idle": "2024-06-15T11:14:21.117997Z",
     "shell.execute_reply": "2024-06-15T11:14:21.117224Z",
     "shell.execute_reply.started": "2024-06-15T11:14:21.108233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create retriver and retrival chain and merger retriever and llm document chain\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain= create_retrieval_chain(retriever=retriever,combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:14:21.119617Z",
     "iopub.status.busy": "2024-06-15T11:14:21.119231Z",
     "iopub.status.idle": "2024-06-15T11:14:41.760546Z",
     "shell.execute_reply": "2024-06-15T11:14:41.759553Z",
     "shell.execute_reply.started": "2024-06-15T11:14:21.119585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inferencing\n",
    "input_question= \"What is attention?\"\n",
    "result=retrieval_chain.invoke({'input':input_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T11:14:41.76194Z",
     "iopub.status.busy": "2024-06-15T11:14:41.761668Z",
     "iopub.status.idle": "2024-06-15T11:14:41.767437Z",
     "shell.execute_reply": "2024-06-15T11:14:41.766536Z",
     "shell.execute_reply.started": "2024-06-15T11:14:41.761915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(result['answer'].split('\\n\\n\\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5215740,
     "sourceId": 8697005,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "langchain_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
